{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF QA PoC using Llamafile\n",
    "\n",
    "We will use LangChain and Llamafile to create a simple PDF QA PoC. We will use a simple PDF file and a simple question to demonstrate the process. However to do this we need to initialize the environment with a local server using a Llamafile that will interact with the LangChain API.\n",
    "\n",
    "We will run one server for embeddings and one for chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile not found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TinyLlama-1.1B-Chat 100%[===================>] 774.20M  11.1MB/s    in 69s     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings server started with PID 77029\n",
      "Logs are being written to embeddings.log\n",
      "Chat server started with PID 77030\n",
      "Logs are being written to chat.log\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "LLAMAFILE=\"TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\"\n",
    "\n",
    "# Check if the file exists\n",
    "if [ ! -f $LLAMAFILE ]; then\n",
    "    echo \"File $LLAMAFILE not found!\"\n",
    "    # Download the file\n",
    "    wget -q --show-progress --progress=bar:force https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/$LLAMAFILE\n",
    "\n",
    "    # Make the file executable\n",
    "    chmod +x $LLAMAFILE\n",
    "fi\n",
    "\n",
    "# Run the file as a embeddings server\n",
    "./$LLAMAFILE --server --nobrowser --embedding --port 8080 > embeddings.log 2>&1 &\n",
    "pid=$!\n",
    "echo \"Embeddings server started with PID $pid\"\n",
    "echo \"Logs are being written to embeddings.log\"\n",
    "echo \"$pid\" > embeddings.pid\n",
    "\n",
    "# Run the file as a chat server\n",
    "./$LLAMAFILE --server --nobrowser --port 8081 > chat.log 2>&1 &\n",
    "pid=$!\n",
    "echo \"Chat server started with PID $pid\"\n",
    "echo \"Logs are being written to chat.log\"\n",
    "echo \"$pid\" > chat.pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to setup langchain to interact with the servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Stock prices quickly incorporate information from earnings\n",
      "announcements, making it difficult to beat the market by\n",
      "trading on these events. A replication of Martineau (2022).\n",
      "Efficient-market hypothesis\n",
      "The efficient-market hypothesis (EMH)[a] is\n",
      "a hypothesis in financial econom ics that states\n",
      "that asset prices reflect all available\n",
      "information. A direct implication is that it is\n",
      "impossible to \"beat the market\" consistently on\n",
      "a risk-adjusted basis since market prices should' metadata={'source': 'data/sample_en.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load a PDF document\n",
    "loader = PyPDFLoader(\"data/sample_en.pdf\")\n",
    "# Create a splitter with some overlap\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "# Load the pdf and split it into chunks\n",
    "documents = loader.load_and_split(splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import LlamafileEmbeddings\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=documents, embedding=LlamafileEmbeddings(\n",
    "  base_url=\"http://localhost:8080\"\n",
    "))\n",
    "\n",
    "# Test the vector store\n",
    "# Ask a question to find similar text\n",
    "question = \"Is it possible to know how the market will evolve in the future?\"\n",
    "docs = vector_store.similarity_search(question)\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.llamafile import Llamafile\n",
    "\n",
    "llm = Llamafile(base_url=\"http://localhost:8081\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Marwala argues that the efficient market hypothesis is applicable to AI-based markets,\\nhowever, its applicability is more limited than the one proposed by the EMH. The efficiency of AI-based\\nmarkets arises from the ability of these systems to learn from past data and adapt accordingly, Marwala argues,\\nwhile the efficient market hypothesis applies only to human-produced markets where trading decisions are made\\nby individuals using their knowledge of the past.\\nS2CID 853397 (https://api.semanticscholar.org/CorpusID:853397).\\n\\ni%2Fcbi014). The efficient market hypothesis is relevant to stock trading because it suggests that investors have\\nlimited information when making trading decisions, and thus it should be difficult to determine whether stocks are\\noverpriced or undervalued. The EMH argues against this claim by showing that markets are not inefficient even\\nwhen there is little market information available. This can be seen as an example of the \"invisible hand\" argument that a free market is efficient because it leads to a good allocation of resources and benefits society as a whole, including investors who use efficient decision-making strategies based on information available at the time they make their decisions.\\nS2CID 853396 (https://api.semanticscholar.org/CorpusID:853396).</s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt\n",
    "prompt = PromptTemplate.from_template(\"Summarize the following text in 3 sentences: {docs}\")\n",
    "\n",
    "# Next we define the chain using a utility function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain = { \"docs\": format_docs } | prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run the chain with the question and docs we have from above\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# cleanup: kill the llamafile server processes\n",
    "\n",
    "kill $(cat chat.pid)\n",
    "rm chat.pid\n",
    "\n",
    "kill $(cat embeddings.pid)\n",
    "rm embeddings.pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-qa-poc-gMiDQn_d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
